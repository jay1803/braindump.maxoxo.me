+++
title = "谷歌内部文件遭泄露｜我们没有壁垒，OpenAI也没有"
author = ["Max Zhang"]
tags = ["ref"]
draft = false
+++

## About {#about}

-   [Google]({{< relref "20220129140242-google.md" >}})
-   [OpenAI]({{< relref "20230327162227-openai.md" >}})

原文：<https://www.semianalysis.com/p/google-we-have-no-moat-and-neither>


## 谷歌内部文件遭泄露｜我们没有壁垒，OpenAI也没有 {#谷歌内部文件遭泄露-我们没有壁垒-openai也没有}

开源模型更快、更可定制、更具隐私性，性能更强。他们用100美元和130亿参数做到了我们在1000万美元和5400亿参数下仍在努力的事情。

一种称为低秩适应[（Low rank adaptation]({{< relref "20230505133737-low_rank_adaptation_of_large_language_models.md" >}})，LoRA）的大大降低成本的微调机制，结合了规模方面的重大突破（图像合成的Latent Diffusion，LLM的Chinchilla）。在这两种情况下，获得足够高质量模型的访问引发了来自世界各地个人和机构的大量想法和迭代。在这两种情况下，这些迅速超过了大型参与者。

LoRA通过将模型更新表示为低秩分解（low-rank factorizations）来工作，从而将更新矩阵的大小减小多达数千倍。这使得模型微调的成本和时间大大减少。

相比之下，从头开始训练巨型模型不仅会丢弃预训练，还会丢弃已经在顶部进行的迭代改进。

[LoRA]({{< relref "20230505133737-low_rank_adaptation_of_large_language_models.md" >}}) 更新对于最受欢迎的模型尺寸非常便宜（约100美元）。


### 时间线 {#时间线}

2023年2月24日 - LLaMA发布

[Meta]({{< relref "20211230140824-facebook.md" >}})发布LLaMA，开源代码，但不开源权重。此时，LLaMA尚未进行指令或对话调优。与许多当前模型一样，它是一个相对较小的模型（参数分别为7B、13B、33B和65B），经过相对较长时间的训练，因此与其大小相比具有相当强大的能力。

2023年3月3日 - 不可避免的事情发生了

一周内，LLaMA被泄露给公众。这对社区的影响不可估量。虽然现有许可证阻止了它被用于商业目的，但突然之间任何人都可以尝试进行实验。从此时开始，创新进展迅速。

2023年3月12日 - 语言模型在烤箱上运行

一个多星期后，Artem Andreenko让模型在树莓派上运行。此时，模型运行速度太慢，因为权重必须在内存中分页，实用性不强。尽管如此，这为一系列的模型缩小工作奠定了基础。

2023年3月13日 - 笔记本电脑上的微调

第二天，斯坦福大学发布了Alpaca，为LLaMA增加了指令调优。然而，更重要的是Eric Wang的alpaca-lora仓库，它使用低秩微调在“单个RTX 4090上几小时内”完成了这个训练。

突然之间，任何人都可以微调模型以执行任何操作，这引发了一场关于低成本微调项目的竞争。论文中自豪地描述了他们总共花费了几百美元。更重要的是，低秩更新可以轻松地与原始权重分开分发，使它们摆脱了Meta原始许可的约束。任何人都可以分享和应用它们。

2023年3月18日 - 现在它变快了

Georgi Gerganov使用4位量化使LLaMA在MacBook CPU上运行。这是第一个“无GPU”解决方案，速度足够快，实用性很强。

2023年3月19日 - 一台13B模型实现了与Bard的“平衡”

第二天，一个跨大学合作推出了Vicuna，并使用GPT-4驱动的评估来提供模型输出的定性比较。虽然评估方法值得怀疑，但该模型比早期版本的表现要好得多。培训成本：300美元。

值得注意的是，他们能够使用来自ChatGPT的数据，同时规避其API的限制 - 他们只需从像ShareGPT这样的网站上获取“令人印象深刻”的ChatGPT对话样本。

2023年3月25日 - 选择您自己的模型

Nomic创建了GPT4All，这既是一个模型，更重要的是一个生态系统。这是我们第一次看到模型（包括Vicuna）聚集在一个地方。培训成本：100美元。

2023年3月28日 - 开源GPT-3

Cerebras（不要与我们自己的Cerebra混淆）使用Chinchilla暗示的最佳计算计划（optimal compute schedule）和μ参数化（u-parameterization）暗示的最佳缩放（optimal scaling）来训练GPT-3架构。这在很大程度上优于现有的GPT-3克隆，并且代表了μ参数化在实际应用中的首次确认使用。这些模型是从零开始训练的，这意味着社区不再依赖于LLaMA。

2023年3月28日 - 一小时内完成多模态训练

使用一种新颖的参数有效微调（PEFT）技术，LLaMA-Adapter在一个小时内引入了指令调优和多模态。令人印象深刻的是，他们只用了120万可学习参数。该模型在多模态ScienceQA上刷新了新的SOTA。

2023年4月3日 - 真实的人类无法分辨13B开放模型和ChatGPT之间的区别

伯克利推出了Koala，这是一个完全使用免费可用数据训练的对话模型。

他们采取了衡量真实人类在他们的模型和ChatGPT之间的偏好的关键步骤。虽然ChatGPT仍然稍占上风，但超过50%的时间，用户要么更喜欢Koala，要么无所谓。培训成本：100美元。

2023年4月15日 - 开源RLHF达到ChatGPT水平

Open Assistant发布了一个模型，更重要的是，发布了一个用于通过RLHF进行对齐的数据集。在人类偏好方面，他们的模型接近ChatGPT（48.3%与51.7%）。除了LLaMA之外，他们展示了该数据集可以应用于Pythia-12B，为人们提供了使用完全开放堆栈运行模型的选择。此外，由于数据集是公开可用的，它使得对于小型实验者来说，RLHF从不可实现变得便宜且容易。
